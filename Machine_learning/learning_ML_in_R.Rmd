---
title: "Using Iris dataset to explore machine learning in R"
author: James Twose
output: html_notebook
---


```{r, echo=FALSE, message=FALSE}
# Activate the R packages
library("dplyr")
library("faux")
library("DataExplorer")
library("caret")
library("randomForest")
library("tidyr")
library("cvms")
```

```{r}
sessionInfo()
```


```{r}
df <- iris
```

```{r}
head(df)

str(df)
```

```{r}
# Exploratory data analysis
plot_intro(df)
plot_bar(df)
plot_correlation(df)
```

```{r}
# Define the control using a random forest selection function
control <- rfeControl(functions = rfFuncs, # random forest
                      method = "repeatedcv", # or just cv
                      repeats = 10, # number of repeats
                      number = 10) # the number of folds
```

```{r}
x <- df %>%
  select(-Species) %>%
  as.data.frame()

y <- df$Species
```

```{r}
set.seed(2021)
inTrain <- createDataPartition(y, p = .80, list = FALSE)[,1]

x_train <- x[ inTrain, ]
x_test  <- x[-inTrain, ]

y_train <- y[ inTrain]
y_test  <- y[-inTrain]
```

```{r}
# Run RFE
result_rfe1 <- rfe(x = x_train, 
                   y = y_train, 
                   sizes = c(1:13), 
                   rfeControl = control)
```

```{r}
# Print the results
result_rfe1
```


```{r}
# Predictors
predictors(result_rfe1)
```


```{r}
# Variable importance
varImp(result_rfe1)
```


```{r}
varimp_data <- data.frame(feature = row.names(varImp(result_rfe1)),
                          importance = varImp(result_rfe1)[, 1])

ggplot(data = varimp_data, 
       aes(x = reorder(feature, -importance), y = importance, fill = feature)) +
  geom_bar(stat="identity") + labs(x = "Features", y = "Variable Importance") + 
  geom_text(aes(label = round(importance, 2)), vjust=1.6, color="white", size=4) + 
  theme_bw() + theme(legend.position = "none")
```
```{r}
# Visualize the results
ggplot(data = result_rfe1, metric = "Accuracy") + theme_bw()
# ggplot(data = result_rfe1, metric = "Kappa") + theme_bw()
```

```{r}
# Post prediction
postResample(predict(result_rfe1, x_test), y_test)
```


```{r}
data.frame(result_rfe1$fit$confusion)
```

```{r}
# Heatmap 

# heatmap(x = cm$table)

# ggplot(data.frame(result_rfe1$fit$confusion), aes(X, Y, fill= Z)) + 
#   geom_tile()
```

```{r}
(result_rfe1$fit$confusion[,-4])
```

```{r}
prediction_tibble <- tibble("target"=y_test,
       "prediction"=predict(result_rfe1, x_test)$pred)
prediction_table <- table(prediction_tibble)
cfm <- as_tibble(prediction_table)
plot_confusion_matrix(cfm, 
                      target_col = "target", 
                      prediction_col = "prediction",
                      counts_col = "n")
```

```{r}
getModelInfo(model="rpart2")
```



```{r}
# Specify 10 fold cross-validation
ctrl_cv <- trainControl(method = "repeatedcv",
                        search="grid",
                        number = 10,
                        repeats=10,
                        timingSamps = 5,
                        seeds = c(1:101)
                        )
# Predict income using decision tree
dec_mod <- train(x=x_train,
                 y=y_train,
                    method = "rpart2",  
                    trControl = ctrl_cv,
                    tuneGrid = expand.grid(
                      maxdepth = c(1, 2, 5 
                                   # 10, 50, 100
                                   )
                      # split = c(1),
                      # prune = c(1, 2)
                      )

                 )
```

```{r}
dec_mod$results
```

```{r}
# Specify 10 fold cross-validation
ctrl_cv <- trainControl(method = "repeatedcv",
                        search="grid",
                        number = 10,
                        repeats=10,
                        timingSamps = 5,
                        # seeds = c(1:101)
                        )
# Predict income using decision tree
dec_mod <- train(x=x_train,
                 y=y_train,
                    method = "rpartScore",  
                    trControl = ctrl_cv,
                    tuneGrid = expand.grid(
                      cp = seq(0,1,0.1),
                      split = c("abs", "quad"),
                      prune = c("mc", "mr")
                      )

                 )
```

```{r}
dec_mod$results
```